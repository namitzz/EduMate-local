# EduMate Environment Configuration Example
# Copy this file to .env and adjust as needed

# ============================================
# Backend Configuration
# ============================================

# ============================================
# LLM Provider Selection
# ============================================

# USE_OPENAI: Choose between OpenRouter (cloud) and Ollama (local)
# Set to 1 to use OpenRouter (OpenAI-compatible API) in production
# Set to 0 or leave unset to use local Ollama (default for development)
USE_OPENAI=0

# ============================================
# OpenRouter Configuration (USE_OPENAI=1)
# ============================================

# OpenRouter API Key (required when USE_OPENAI=1)
# Get your API key from: https://openrouter.ai/keys
# OPENAI_API_KEY=sk-or-v1-your-api-key-here

# OpenRouter Base URL (OpenAI-compatible)
# Default: https://openrouter.ai/api/v1
# OPENAI_BASE_URL=https://openrouter.ai/api/v1

# Model to use with OpenRouter
# Examples: openai/gpt-3.5-turbo, openai/gpt-4, anthropic/claude-3-haiku, etc.
# See available models: https://openrouter.ai/models
# OPENAI_MODEL=openai/gpt-3.5-turbo

# ============================================
# Ollama Configuration (USE_OPENAI=0)
# ============================================

# Fast Mode (set to 1 to enable optimizations for 6-student pilot)
# When enabled: smaller chunks, fewer retrievals, faster responses
# Default: 1 (enabled) for optimal performance
FAST_MODE=1

# Ollama Configuration
# ─────────────────────────────────────────────────────────────────
# IMPORTANT: Set OLLAMA_URL based on your deployment type:
#
# 1. LOCAL DEVELOPMENT (non-Docker):
#    OLLAMA_URL=http://localhost:11434
#    → Use this when running Ollama on your local machine
#    → Default if not set
#
# 2. DOCKER DEPLOYMENT:
#    OLLAMA_HOST=http://ollama:11434
#    → Set in docker-compose.yml (auto-configured)
#    → Uses container-to-container networking
#
# 3. CLOUD/PUBLIC API DEPLOYMENT (Fly.io, Streamlit Cloud, Railway, etc.):
#    OLLAMA_URL=https://your-ollama-api-endpoint.com
#    → Use your assigned public/shared Ollama API endpoint
#    → Examples: https://api.ollama.ai, https://your-instance.fly.dev
#    → Do NOT use localhost - it won't work in cloud deployments
#    → Check if your provider requires API key authentication
#
# For local development, use the default:
OLLAMA_URL=http://localhost:11434

# Model to use (smaller models = faster responses)
OLLAMA_MODEL=mistral:latest

# For faster responses in pilot, consider a smaller model:
# OLLAMA_MODEL=qwen2.5:1.5b-instruct

# ============================================
# Generation Settings (applies to both providers)
# ============================================

# Generation Settings
MAX_ACTIVE_GENERATIONS=1  # Concurrency limit (1 = sequential)
TEMP=0.3                  # Temperature for generation (0.0-1.0)
NUM_PREDICT=400           # Max tokens to generate (default 400 for faster responses)

# ============================================
# UI Configuration
# ============================================

# Backend API URL for the UI to connect to
EDUMATE_API_URL=http://localhost:8000
# For Docker: EDUMATE_API_URL=http://backend:8000

# Alternative name for API base (backward compatibility)
API_BASE=http://localhost:8000

# ============================================
# Docker Compose
# ============================================

COMPOSE_PROJECT_NAME=edumate-local
